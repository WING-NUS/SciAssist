{"summary": ["This paper proposes a bidirectional language model pre-training method for sentence-level tasks. The main idea is to use a masked language model to fuse the left and the right context, which allows the model to learn bidirectionally from both directions. The authors also propose a next sentence prediction task to jointly pre-train text-pair representations. The proposed method is evaluated on a suite of sentence- and token-level task and achieves state-of-the-art performance."], "raw_text": "Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce \ufb01ne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and \ufb01ne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speci\ufb01c architectures thatinclude the pre-trained representations as addi-tional features. The \ufb01ne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speci\ufb01c parameters, and is trained on thedownstream tasks by simply \ufb01ne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the \ufb01ne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying \ufb01ne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the \ufb01ne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a \u201cmasked lan-guage model\u201d (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea \u201cnext sentence prediction\u201d task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: \u2022 We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. \u2022 We show that pre-trained representations reducethe need for many heavily-engineered task-speci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speci\ufb01c architectures. \u2022 BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert. 2 Related Work There is a long history of pre-training general lan-guage representations, and we brie\ufb02y review themost widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations ofwords has been an active area of research fordecades, including non-neural (Brown et al., 1992;Ando and Zhang, 2005; Blitzer et al., 2006) andneural (Mikolov et al., 2013; Pennington et al.,2014) methods.Pre-trained word embeddingsare an integral part of modern NLP systems, of-fering signi\ufb01cant improvements over embeddingslearned from scratch (Turian et al., 2010). To pre-train word embedding vectors, left-to-right lan-guage modeling objectives have been used (Mnihand Hinton, 2009), as well as objectives to dis-criminate correct from incorrect words in left andright context (Mikolov et al., 2013). These approaches have been generalized tocoarser granularities, such as sentence embed-dings (Kiros et al., 2015; Logeswaran and Lee,2018) or paragraph embeddings (Le and Mikolov,2014). To train sentence representations, priorwork has used objectives to rank candidate nextsentences (Jernite et al., 2017; Logeswaran andLee, 2018), left-to-right generation of next sen-tence words given a representation of the previoussentence (Kiros et al., 2015), or denoising auto-encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017,2018a) generalize traditional word embedding re-search along a different dimension. They extractcontext-sensitive features from a left-to-right and aright-to-left language model. The contextual rep-resentation of each token is the concatenation ofthe left-to-right and right-to-left representations.When integrating contextual word embeddingswith existing task-speci\ufb01c architectures, ELMoadvances the state of the art for several major NLPbenchmarks (Peters et al., 2018a) including ques-tion answering (Rajpurkar et al., 2016), sentimentanalysis (Socher et al., 2013), and named entityrecognition (Tjong Kim Sang and De Meulder,2003). Melamud et al. (2016) proposed learningcontextual representations through a task to pre-dict a single word from both left and right contextusing LSTMs. Similar to ELMo, their model isfeature-based and not deeply bidirectional. Feduset al. (2018) shows that the cloze task can be usedto improve the robustness of text generation mod-els. 2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the \ufb01rstworks in this direction only pre-trained word em-(Col-bedding parameters from unlabeled textlobert and Weston, 2008). More recently, sentence or document encoderswhich produce contextual token representationshave been pre-trained from unlabeled text and\ufb01ne-tuned for a supervised downstream task (Daiand Le, 2015; Howard and Ruder, 2018; Radfordet al., 2018). The advantage of these approachesis that few parameters need to be learned fromscratch. At least partly due to this advantage,OpenAI GPT (Radford et al., 2018) achieved pre-viously state-of-the-art results on many sentence-level tasks from the GLUE benchmark (Wanglanguage model-Left-to-rightet al., 2018a). Figure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-tures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initializemodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a specialsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-tions/answers). ing and auto-encoder objectives have been usedfor pre-training such models (Howard and Ruder,2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans-fer from supervised tasks with large datasets, suchas natural language inference (Conneau et al.,2017) and machine translation (McCann et al.,2017). Computer vision research has also demon-strated the importance of transfer learning fromlarge pre-trained models, where an effective recipeis to \ufb01ne-tune models pre-trained with Ima-geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa-tion in this section. There are two steps in ourframework: pre-training and \ufb01ne-tuning. Dur-ing pre-training, the model is trained on unlabeleddata over different pre-training tasks. For \ufb01ne-tuning, the BERT model is \ufb01rst initialized withthe pre-trained parameters, and all of the param-eters are \ufb01ne-tuned using labeled data from thedownstream tasks. Each downstream task has sep-arate \ufb01ne-tuned models, even though they are ini-tialized with the same pre-trained parameters. Thequestion-answering example in Figure 1 will serveas a running example for this section. A distinctive feature of BERT is its uni\ufb01ed ar-chitecture across different tasks. There is mini- mal difference between the pre-trained architec-ture and the \ufb01nal downstream architecture. Model Architecture BERT\u2019s model architec-ture is a multi-layer bidirectional Transformer en-coder based on the original implementation de-scribed in Vaswani et al. (2017) and released inthe tensor2tensor library.1 Because the useof Transformers has become common and our im-plementation is almost identical to the original,we will omit an exhaustive background descrip-tion of the model architecture and refer readers toVaswani et al. (2017) as well as excellent guidessuch as \u201cThe Annotated Transformer.\u201d2 In this work, we denote the number of layers(i.e., Transformer blocks) as L, the hidden size asH, and the number of self-attention heads as A.3We primarily report results on two model sizes:BERTBASE (L=12, H=768, A=12, Total Param-eters=110M) and BERTLARGE (L=24, H=1024,A=16, Total Parameters=340M). BERTBASE was chosen to have the same modelsize as OpenAI GPT for comparison purposes.Critically, however, the BERT Transformer usesbidirectional self-attention, while the GPT Trans-former uses constrained self-attention where everytoken can only attend to context to its left.4 1https://github.com/tensor\ufb02ow/tensor2tensor2http://nlp.seas.harvard.edu/2018/04/03/attention.html3In all cases we set the feed-forward/\ufb01lter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERThandle a variety of down-stream tasks, our inputrepresentation is able to unambiguously representboth a single sentence and a pair of sentences(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.Throughout this work, a \u201csentence\u201d can be an arbi-trary span of contiguous text, rather than an actuallinguistic sentence. A \u201csequence\u201d refers to the in-put token sequence to BERT, which may be a sin-gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al.,2016) with a 30,000 token vocabulary. The \ufb01rsttoken of every sequence is always a special clas-si\ufb01cation token ([CLS]). The \ufb01nal hidden statecorresponding to this token is used as the ag-gregate sequence representation for classi\ufb01cationtasks. Sentence pairs are packed together into asingle sequence. We differentiate the sentences intwo ways. First, we separate them with a specialtoken ([SEP]). Second, we add a learned embed-ding to every token indicating whether it belongsto sentence A or sentence B. As shown in Figure 1,we denote input embedding as E, the \ufb01nal hiddenvector of the special [CLS] token as C \u2208 RH ,and the \ufb01nal hidden vector for the ith input tokenas Ti \u2208 RH . For a given token, its input representation isconstructed by summing the corresponding token,segment, and position embeddings. A visualiza-tion of this construction can be seen in Figure 2. 3.1 Pre-training BERT Unlike Peters et al. (2018a) and Radford et al.(2018), we do not use traditional left-to-right orright-to-left language models to pre-train BERT.Instead, we pre-train BERT using two unsuper-vised tasks, described in this section. This stepis presented in the left part of Figure 1. Task #1: Masked LM Intuitively, it is reason-able to believe that a deep bidirectional model isstrictly more powerful than either a left-to-rightmodel or the shallow concatenation of a left-to-right and a right-to-left model. Unfortunately,standard conditional language models can only betrained left-to-right or right-to-left, since bidirec-tional conditioning would allow each word to in-directly \u201csee itself\u201d, and the model could triviallypredict the target word in a multi-layered context. In order to train a deep bidirectional representa-tion, we simply mask some percentage of the inputtokens at random, and then predict those maskedtokens. We refer to this procedure as a \u201cmaskedLM\u201d (MLM), although it is often referred to as aCloze task in the literature (Taylor, 1953). In thiscase, the \ufb01nal hidden vectors corresponding to themask tokens are fed into an output softmax overthe vocabulary, as in a standard LM. In all of ourexperiments, we mask 15% of all WordPiece to-kens in each sequence at random. In contrast todenoising auto-encoders (Vincent et al., 2008), weonly predict the masked words rather than recon-structing the entire input. Although this allows us to obtain a bidirec-tional pre-trained model, a downside is that weare creating a mismatch between pre-training and\ufb01ne-tuning, since the [MASK] token does not ap-pear during \ufb01ne-tuning. To mitigate this, we donot always replace \u201cmasked\u201d words with the ac-tual [MASK] token. The training data generatorchooses 15% of the token positions at random forprediction. If the i-th token is chosen, we replacethe i-th token with (1) the [MASK] token 80% ofthe time (2) a random token 10% of the time (3)the unchanged i-th token 10% of the time. Then,Ti will be used to predict the original token withcross entropy loss. We compare variations of thisprocedure in Appendix C.2. Task #2: Next Sentence Prediction (NSP)Many important downstream tasks such as Ques-tion Answering (QA) and Natural Language Infer-ence (NLI) are based on understanding the rela-tionship between two sentences, which is not di-rectly captured by language modeling.In orderto train a model that understands sentence rela-tionships, we pre-train for a binarized next sen-tence prediction task that can be trivially gener-ated from any monolingual corpus. Speci\ufb01cally,when choosing the sentences A and B for each pre-training example, 50% of the time B is the actualnext sentence that follows A (labeled as IsNext),and 50% of the time it is a random sentence fromthe corpus (labeled as NotNext). As we showin Figure 1, C is used for next sentence predic-tion (NSP).5 Despite its simplicity, we demon-strate in Section 5.1 that pre-training towards thistask is very bene\ufb01cial to both QA and NLI. 6 former is often referred to as a \u201cTransformer encoder\u201d whilethe left-context-only version is referred to as a \u201cTransformerdecoder\u201d since it can be used for text generation. 5The \ufb01nal model achieves 97%-98% accuracy on NSP.6The vector C is not a meaningful sentence representation without \ufb01ne-tuning, since it was trained with NSP. Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-tion embeddings and the position embeddings. The NSP task is closely related to representation-learning objectives used in Jernite et al. (2017) andLogeswaran and Lee (2018). However, in priorwork, only sentence embeddings are transferred todown-stream tasks, where BERT transfers all pa-rameters to initialize end-task model parameters. Pre-training data The pre-training procedurelargely follows the existing literature on languagemodel pre-training. For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al.,2015) and English Wikipedia (2,500M words).For Wikipedia we extract only the text passagesand ignore lists, tables, and headers.It is criti-cal to use a document-level corpus rather than ashuf\ufb02ed sentence-level corpus such as the BillionWord Benchmark (Chelba et al., 2013) in order toextract long contiguous sequences. 3.2 Fine-tuning BERT Fine-tuning is straightforward since the self-attention mechanism in the Transformer al-lows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014byswapping out the appropriate inputs and outputs.For applications involving text pairs, a commonpattern is to independently encode text pairs be-fore applying bidirectional cross attention, suchas Parikh et al. (2016); Seo et al. (2017). BERTinstead uses the self-attention mechanism to unifythese two stages, as encoding a concatenated textpair with self-attention effectively includes bidi-rectional cross attention between two sentences. For each task, we simply plug in the task-speci\ufb01c inputs and outputs into BERT and \ufb01ne-tune all the parameters end-to-end. At the in-put, sentence A and sentence B from pre-trainingare analogous to (1) sentence pairs in paraphras-ing, (2) hypothesis-premise pairs in entailment, (3)question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classi\ufb01cationor sequence tagging. At the output, the token rep-resentations are fed into an output layer for token-level tasks, such as sequence tagging or questionanswering, and the [CLS] representation is fedinto an output layer for classi\ufb01cation, such as en-tailment or sentiment analysis. Compared to pre-training, \ufb01ne-tuning is rela-tively inexpensive. All of the results in the pa-per can be replicated in at most 1 hour on a sin-gle Cloud TPU, or a few hours on a GPU, startingfrom the exact same pre-trained model.7 We de-scribe the task-speci\ufb01c details in the correspond-ing subsections of Section 4. More details can befound in Appendix A.5. 4 Experiments In this section, we present BERT \ufb01ne-tuning re-sults on 11 NLP tasks. 4.1 GLUE The General Language Understanding Evaluation(GLUE) benchmark (Wang et al., 2018a) is a col-lection of diverse natural language understandingtasks. Detailed descriptions of GLUE datasets areincluded in Appendix B.1. To \ufb01ne-tune on GLUE, we represent the inputsequence (for single sentence or sentence pairs)as described in Section 3, and use the \ufb01nal hid-den vector C \u2208 RH corresponding to the \ufb01rstinput token ([CLS]) as the aggregate representa-tion. The only new parameters introduced during\ufb01ne-tuning are classi\ufb01cation layer weights W \u2208RK\u00d7H , where K is the number of labels. We com-pute a standard classi\ufb01cation loss with C and W ,i.e., log(softmax(CW T )). 7For example, the BERT SQuAD model can be trained inaround 30 minutes on a single Cloud TPU to achieve a DevF1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq. System Pre-OpenAI SOTABiLSTM+ELMo+AttnOpenAI GPTBERTBASEBERTLARGE MNLI-(m/mm)392k80.6/80.176.4/76.182.1/81.484.6/83.486.7/85.9 QQP363k66.164.870.371.272.1 QNLI108k82.379.887.490.592.7 SST-267k93.290.491.393.594.9 CoLA8.5k35.036.045.452.160.5 STS-B MRPC5.7k81.073.380.085.886.5 3.5k86.084.982.388.989.3 RTE2.5k61.756.856.066.470.1 Average-74.071.075.179.682.1 Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly differentthan the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, andaccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and \ufb01ne-tune for 3epochs over the data for all GLUE tasks. For eachtask, we selected the best \ufb01ne-tuning learning rate(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.Additionally, for BERTLARGE we found that \ufb01ne-tuning was sometimes unstable on small datasets,so we ran several random restarts and selected thebest model on the Dev set. With random restarts,we use the same pre-trained checkpoint but per-form different \ufb01ne-tuning data shuf\ufb02ing and clas-si\ufb01er layer initialization.9 Results are presented in Table 1. BothBERTBASE and BERTLARGE outperform all sys-tems on all tasks by a substantial margin, obtaining4.5% and 7.0% respective average accuracy im-provement over the prior state of the art. Note thatBERTBASE and OpenAI GPT are nearly identicalin terms of model architecture apart from the at-tention masking. For the largest and most widelyreported GLUE task, MNLI, BERT obtains a 4.6%absolute accuracy improvement. On the of\ufb01cialGLUE leaderboard10, BERTLARGE obtains a scoreof 80.5, compared to OpenAI GPT, which obtains72.8 as of the date of writing. We \ufb01nd that BERTLARGE signi\ufb01cantly outper-forms BERTBASE across all tasks, especially thosewith very little training data. The effect of modelsize is explored more thoroughly in Section 5.2. 4.2 SQuAD v1.1 The Stanford Question Answering Dataset(SQuAD v1.1) is a collection of 100k crowd-sourced question/answer pairs (Rajpurkar et al.,2016). Given a question and a passage from 9The GLUE data set distribution does not include the Testlabels, and we only made a single GLUE evaluation serversubmission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Wikipedia containing the answer, the task is topredict the answer text span in the passage. As shown in Figure 1, in the question answer-ing task, we represent the input question and pas-sage as a single packed sequence, with the ques-tion using the A embedding and the passage usingthe B embedding. We only introduce a start vec-tor S \u2208 RH and an end vector E \u2208 RH during\ufb01ne-tuning. The probability of word i being thestart of the answer span is computed as a dot prod-uct between Ti and S followed by a softmax overall of the words in the paragraph: Pi = eS\u00b7Tij eS\u00b7Tj .The analogous formula is used for the end of theanswer span. The score of a candidate span fromposition i to position j is de\ufb01ned as S\u00b7Ti + E\u00b7Tj,and the maximum scoring span where j \u2265 i isused as a prediction. The training objective is thesum of the log-likelihoods of the correct start andend positions. We \ufb01ne-tune for 3 epochs with alearning rate of 5e-5 and a batch size of 32. (cid:80) Table 2 shows top leaderboard entries as wellas results from top published systems (Seo et al.,2017; Clark and Gardner, 2018; Peters et al.,2018a; Hu et al., 2018). The top results from theSQuAD leaderboard do not have up-to-date publicsystem descriptions available,11 and are allowed touse any public data when training their systems.We therefore use modest data augmentation inour system by \ufb01rst \ufb01ne-tuning on TriviaQA (Joshiet al., 2017) befor \ufb01ne-tuning on SQuAD. Our best performing system outperforms the topleaderboard system by +1.5 F1 in ensembling and+1.3 F1 as a single system.In fact, our singleBERT model outperforms the top ensemble sys-tem in terms of F1 score. Without TriviaQA \ufb01ne- 11QANet is described in Yu et al. (2018), but the system has improved substantially after publication. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human#1 Ensemble - nlnet#2 Ensemble - QANet --- --- 82.3 91.286.0 91.784.5 90.5 Published BiDAF+ELMo (Single)R.M. Reader (Ensemble) Ours - 85.6 85.881.2 87.9 82.3 88.5 - BERTBASE (Single)BERTLARGE (Single)BERTLARGE (Ensemble)BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 80.8 88.584.1 90.985.8 91.8 --- --- Table 2: SQuAD 1.1 results. The BERT ensembleis 7x systems which use different pre-training check-points and \ufb01ne-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human#1 Single - MIR-MRC (F-Net)#2 Single - nlnet 86.3 89.0 86.9 89.574.8 78.074.2 77.1 -- -- unet (Ensemble)SLQA+ (Single) Published Ours -- - 71.4 74.971.4 74.4 BERTLARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries thatuse BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper-forming all existing systems by a wide margin.12 4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1problem de\ufb01nition by allowing for the possibilitythat no short answer exists in the provided para-graph, making the problem more realistic. We use a simple approach to extend the SQuADv1.1 BERT model for this task. We treat ques-tions that do not have an answer as having an an-swer span with start and end at the [CLS] to-ken. The probability space for the start and endanswer span positions is extended to include theposition of the [CLS] token. For prediction, wecompare the score of the no-answer span: snull =S\u00b7C + E\u00b7C to the score of the best non-null span 12The TriviaQA data we used consists of paragraphs fromTriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents,that contain at least one of the provided possible answers. System ESIM+GloVeESIM+ELMoOpenAI GPT BERTBASEBERTLARGE Dev Test 51.9 52.759.1 59.278.0 - - 81.686.6 86.3 Human (expert)\u2020Human (5 annotations)\u2020 -- 85.088.0 Table 4: SWAG Dev and Test accuracies. \u2020Human per-formance is measured with 100 samples, as reported inthe SWAG paper. \u02c6si,j = maxj\u2265iS\u00b7Ti + E\u00b7Tj. We predict a non-nullanswer when \u02c6si,j > snull + \u03c4 , where the thresh-old \u03c4 is selected on the dev set to maximize F1.We did not use TriviaQA data for this model. We\ufb01ne-tuned for 2 epochs with a learning rate of 5e-5and a batch size of 48. The results compared to prior leaderboard en-tries and top published work (Sun et al., 2018;Wang et al., 2018b) are shown in Table 3, exclud-ing systems that use BERT as one of their com-ponents. We observe a +5.1 F1 improvement overthe previous best system. 4.4 SWAG The Situations With Adversarial Generations(SWAG) dataset contains 113k sentence-pair com-pletion examples that evaluate grounded common-sense inference (Zellers et al., 2018). Given a sen-tence, the task is to choose the most plausible con-tinuation among four choices. When \ufb01ne-tuning on the SWAG dataset, weconstruct four input sequences, each containingthe concatenation of the given sentence (sentenceA) and a possible continuation (sentence B). Theonly task-speci\ufb01c parameters introduced is a vec-tor whose dot product with the [CLS] token rep-resentation C denotes a score for each choicewhich is normalized with a softmax layer. We \ufb01ne-tune the model for 3 epochs with alearning rate of 2e-5 and a batch size of 16. Re-sults are presented in Table 4. BERTLARGE out-performs the authors\u2019 baseline ESIM+ELMo sys-tem by +27.1% and OpenAI GPT by 8.3%. 5 Ablation Studies In this section, we perform ablation experimentsover a number of facets of BERT in order to betterunderstand their relative importance. Additional Tasks Dev SetMNLI-m QNLI MRPC SST-2 SQuAD(Acc) (Acc) (Acc) (Acc) (F1) BERTBASENo NSPLTR & No NSP+ BiLSTM 84.483.982.182.1 88.484.984.384.1 86.786.577.575.7 92.792.692.191.6 88.587.977.884.9 Table 5: Ablation over the pre-training tasks using theBERTBASE architecture. \u201cNo NSP\u201d is trained withoutthe next sentence prediction task. \u201cLTR & No NSP\u201d istrained as a left-to-right LM without the next sentenceprediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran-domly initialized BiLSTM on top of the \u201cLTR + NoNSP\u201d model during \ufb01ne-tuning. results are still far worse than those of the pre-trained bidirectional models. The BiLSTM hurtsperformance on the GLUE tasks. We recognize that it would also be possible totrain separate LTR and RTL models and representeach token as the concatenation of the two mod-els, as ELMo does. However: (a) this is twice asexpensive as a single bidirectional model; (b) thisis non-intuitive for tasks like QA, since the RTLmodel would not be able to condition the answeron the question; (c) this it is strictly less powerfulthan a deep bidirectional model, since it can useboth left and right context at every layer. ablation studies can be found in Appendix C. 5.2 Effect of Model Size 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi-rectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, \ufb01ne-tuning scheme, and hyperpa-rameters as BERTBASE: No NSP: A bidirectional model which is trainedusing the \u201cmasked LM\u201d (MLM) but without the\u201cnext sentence prediction\u201d (NSP) task.LTR & No NSP: A left-context-only model whichis trained using a standard Left-to-Right (LTR)LM, rather than an MLM. The left-only constraintwas also applied at \ufb01ne-tuning, because removingit introduced a pre-train/\ufb01ne-tune mismatch thatdegraded downstream performance. Additionally,this model was pre-trained without the NSP task.This is directly comparable to OpenAI GPT, butusing our larger training dataset, our input repre-sentation, and our \ufb01ne-tuning scheme. We \ufb01rst examine the impact brought by the NSPIn Table 5, we show that removing NSPtask.hurts performance signi\ufb01cantly on QNLI, MNLI,and SQuAD 1.1. Next, we evaluate the impactof training bidirectional representations by com-paring \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTRmodel performs worse than the MLM model on alltasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTRmodel will perform poorly at token predictions,since the token-level hidden states have no right-side context.In order to make a good faith at-tempt at strengthening the LTR system, we addeda randomly initialized BiLSTM on top. This doessigni\ufb01cantly improve results on SQuAD, but the In this section, we explore the effect of model sizeon \ufb01ne-tuning task accuracy. We trained a numberof BERT models with a differing number of layers,hidden units, and attention heads, while otherwiseusing the same hyperparameters and training pro-cedure as described previously. Results on selected GLUE tasks are shown inTable 6. In this table, we report the average DevSet accuracy from 5 random restarts of \ufb01ne-tuning.We can see that larger models lead to a strict ac-curacy improvement across all four datasets, evenfor MRPC which only has 3,600 labeled train-ing examples, and is substantially different fromthe pre-training tasks. It is also perhaps surpris-ing that we are able to achieve such signi\ufb01cantimprovements on top of models which are al-ready quite large relative to the existing literature.For example, the largest Transformer explored inVaswani et al. (2017) is (L=6, H=1024, A=16)with 100M parameters for the encoder, and thelargest Transformer we have found in the literatureis (L=64, H=512, A=2) with 235M parameters(Al-Rfou et al., 2018). By contrast, BERTBASEcontains 110M parameters and BERTLARGE con-tains 340M parameters. It has long been known that increasing themodel size will lead to continual improvementson large-scale tasks such as machine translationand language modeling, which is demonstratedby the LM perplexity of held-out training datashown in Table 6. However, we believe thatthis is the \ufb01rst work to demonstrate convinc-ingly that scaling to extreme model sizes alsoleads to large improvements on very small scaletasks, provided that the model has been suf\ufb01-ciently pre-trained. Peters et al. (2018b) presented mixed results on the downstream task impact ofincreasing the pre-trained bi-LM size from twoto four layers and Melamud et al. (2016) men-tioned in passing that increasing hidden dimen-sion size from 200 to 600 helped, but increasingfurther to 1,000 did not bring further improve-ments. Both of these prior works used a feature-based approach \u2014 we hypothesize that when themodel is \ufb01ne-tuned directly on the downstreamtasks and uses only a very small number of ran-domly initialized additional parameters, the task-speci\ufb01c models can bene\ufb01t from the larger, moreexpressive pre-trained representations even whendownstream task data is very small. 5.3 Feature-based Approach with BERT All of the BERT results presented so far have usedthe \ufb01ne-tuning approach, where a simple classi\ufb01-cation layer is added to the pre-trained model, andall parameters are jointly \ufb01ne-tuned on a down-stream task. However, the feature-based approach,where \ufb01xed features are extracted from the pre-trained model, has certain advantages. First, notall tasks can be easily represented by a Trans-former encoder architecture, and therefore requirea task-speci\ufb01c model architecture to be added.Second, there are major computational bene\ufb01tsto pre-compute an expensive representation of thetraining data once and then run many experimentswith cheaper models on top of this representation.In this section, we compare the two approachesby applying BERT to the CoNLL-2003 NamedEntity Recognition (NER) task (Tjong Kim Sangand De Meulder, 2003). In the input to BERT, weuse a case-preserving WordPiece model, and weinclude the maximal document context providedby the data. Following standard practice, we for-mulate this as a tagging task but do not use a CRF Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 768 12376836768 12612768 1212 1024 1624 1024 16 5.845.244.683.993.543.23 77.980.681.984.485.786.6 79.882.284.886.786.987.8 88.490.791.392.993.393.7 Table 6: Ablation over BERT model size. #L = thenumber of layers; #H = hidden size; #A = number of at-tention heads. \u201cLM (ppl)\u201d is the masked LM perplexityof held-out training data. System Dev F1 Test F1 ELMo (Peters et al., 2018a)CVT (Clark et al., 2018)CSE (Akbik et al., 2018) Fine-tuning approach BERTLARGEBERTBASE Feature-based approach (BERTBASE) EmbeddingsSecond-to-Last HiddenLast HiddenWeighted Sum Last Four HiddenConcat Last Four HiddenWeighted Sum All 12 Layers 95.7-- 96.696.4 91.095.694.995.996.195.5 92.292.693.1 92.892.4 ------ Table 7: CoNLL-2003 Named Entity Recognition re-sults. Hyperparameters were selected using the Devset. The reported Dev and Test scores are averaged over5 random restarts using those hyperparameters. layer in the output. We use the representation ofthe \ufb01rst sub-token as the input to the token-levelclassi\ufb01er over the NER label set. To ablate the \ufb01ne-tuning approach, we apply thefeature-based approach by extracting the activa-tions from one or more layers without \ufb01ne-tuningany parameters of BERT. These contextual em-beddings are used as input to a randomly initial-ized two-layer 768-dimensional BiLSTM beforethe classi\ufb01cation layer. Results are presented in Table 7. BERTLARGEperforms competitively with state-of-the-art meth-ods. The best performing method concatenates thetoken representations from the top four hidden lay-ers of the pre-trained Transformer, which is only0.3 F1 behind \ufb01ne-tuning the entire model. Thisdemonstrates that BERT is effective for both \ufb01ne-tuning and feature-based approaches. 6 Conclusion Recent empirical improvements due to transferlearning with language models have demonstratedthat rich, unsupervised pre-training is an integralpart of many language understanding systems. Inparticular, these results enable even low-resourcetasks to bene\ufb01t from deep unidirectional architec-tures. Our major contribution is further general-izing these \ufb01ndings to deep bidirectional architec-tures, allowing the same pre-trained model to suc-cessfully tackle a broad set of NLP tasks. "}
